{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) Setup\n",
        "import os, sys, json, pickle, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss,\n",
        "    classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
        "    brier_score_loss\n",
        ")\n",
        "from sklearn.calibration import CalibrationDisplay, calibration_curve\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Try optional imports\n",
        "try:\n",
        "    import shap  # type: ignore\n",
        "except Exception:\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb  # type: ignore\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb  # type: ignore\n",
        "except Exception:\n",
        "    lgb = None\n",
        "\n",
        "# Config (EDIT HERE)\n",
        "ARTIFACT_DIR = Path(\"artifacts\")\n",
        "TASK_TYPE = \"classification\"  # or \"regression\"\n",
        "MODEL_FILES = [\n",
        "    ARTIFACT_DIR / \"best_model.joblib\",\n",
        "]\n",
        "MODEL_NAMES = [\n",
        "    \"BestModel\",\n",
        "]\n",
        "# If using test.csv\n",
        "TEST_CSV = None  # e.g., ARTIFACT_DIR / \"test.csv\"\n",
        "TARGET_NAME = \"diabetes\"\n",
        "CLASS_NAMES = [\"0\", \"1\"]\n",
        "\n",
        "# If using npy files\n",
        "X_TEST_NPY = None  # e.g., ARTIFACT_DIR / \"X_test.npy\"\n",
        "Y_TEST_NPY = None  # e.g., ARTIFACT_DIR / \"y_test.npy\"\n",
        "\n",
        "# Output paths\n",
        "OUT_DIR = Path(\"reports\") / \"report_outputs\"\n",
        "PLOTS_DIR = OUT_DIR / \"plots\"\n",
        "TABLES_DIR = OUT_DIR / \"tables\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Utils\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def ensure_dir(p: Path) -> None:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def save_df(name: str, df: pd.DataFrame) -> None:\n",
        "    path = TABLES_DIR / f\"{name}.csv\"\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "def save_fig(name: str) -> None:\n",
        "    path = PLOTS_DIR / f\"{name}.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "print(\"Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Data loading\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "# Allow overriding TEST_CSV via config; default to project dataset if present\n",
        "_default_csv = Path('diabetes_prediction_dataset.csv')\n",
        "if TEST_CSV is None and _default_csv.exists():\n",
        "    TEST_CSV = _default_csv\n",
        "\n",
        "\n",
        "def load_test_data() -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    if X_TEST_NPY and Y_TEST_NPY and Path(X_TEST_NPY).exists() and Path(Y_TEST_NPY).exists():\n",
        "        X = np.load(X_TEST_NPY, allow_pickle=False)\n",
        "        y = np.load(Y_TEST_NPY, allow_pickle=False)\n",
        "        if isinstance(X, np.ndarray) and X.ndim == 2:\n",
        "            X_df = pd.DataFrame(X)\n",
        "        else:\n",
        "            raise ValueError(\"X_test.npy must be a 2D array\")\n",
        "        y_ser = pd.Series(y)\n",
        "        return X_df, y_ser\n",
        "    if TEST_CSV and Path(TEST_CSV).exists():\n",
        "        df = pd.read_csv(TEST_CSV)\n",
        "        if TARGET_NAME not in df.columns:\n",
        "            raise ValueError(f\"TARGET_NAME '{TARGET_NAME}' not in columns of {TEST_CSV}\")\n",
        "        X_df = df.drop(columns=[TARGET_NAME])\n",
        "        y_ser = df[TARGET_NAME]\n",
        "        return X_df, y_ser\n",
        "    raise FileNotFoundError(\"No test data found. Provide X_test.npy/y_test.npy or test.csv (TEST_CSV).\")\n",
        "\n",
        "X_test, y_test = load_test_data()\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"Nulls in X_test:\", int(pd.isna(X_test).sum().sum()))\n",
        "if TASK_TYPE == 'classification':\n",
        "    print(\"Class balance:\")\n",
        "    print(y_test.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Model loading\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def try_load_model(path: Path) -> Any:\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"Model file not found: {path}\")\n",
        "    # joblib\n",
        "    try:\n",
        "        return joblib.load(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # pickle\n",
        "    try:\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # xgboost\n",
        "    if xgb is not None:\n",
        "        try:\n",
        "            booster = xgb.Booster()\n",
        "            booster.load_model(str(path))\n",
        "            return booster\n",
        "        except Exception:\n",
        "            pass\n",
        "    # lightgbm\n",
        "    if lgb is not None:\n",
        "        try:\n",
        "            booster = lgb.Booster(model_file=str(path))\n",
        "            return booster\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise RuntimeError(f\"Unable to load model: {path}\")\n",
        "\n",
        "\n",
        "models = []\n",
        "for i, mf in enumerate(MODEL_FILES):\n",
        "    try:\n",
        "        m = try_load_model(Path(mf))\n",
        "        name = MODEL_NAMES[i] if i < len(MODEL_NAMES) else f\"Model_{i+1}\"\n",
        "        models.append({\"name\": name, \"model\": m})\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: skipping {mf}: {e}\")\n",
        "\n",
        "if not models:\n",
        "    raise RuntimeError(\"No models loaded. Check MODEL_FILES and paths.\")\n",
        "\n",
        "print(\"Loaded models:\", [m[\"name\"] for m in models])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Predictions\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "preds = []\n",
        "for entry in models:\n",
        "    name, model = entry[\"name\"], entry[\"model\"]\n",
        "    y_pred = None\n",
        "    y_proba = None\n",
        "    try:\n",
        "        # scikit-like API\n",
        "        y_pred = model.predict(X_test)\n",
        "        if TASK_TYPE == 'classification':\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                y_proba = model.predict_proba(X_test)\n",
        "            elif hasattr(model, 'decision_function'):\n",
        "                scores = model.decision_function(X_test)\n",
        "                # convert to pseudo-proba (min-max)\n",
        "                scores = np.asarray(scores)\n",
        "                if scores.ndim == 1:\n",
        "                    s = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
        "                    y_proba = np.vstack([1 - s, s]).T\n",
        "                else:\n",
        "                    s = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
        "                    y_proba = s\n",
        "    except Exception:\n",
        "        # xgboost/lightgbm boosters require DMatrix/construct\n",
        "        try:\n",
        "            if xgb is not None and isinstance(model, xgb.Booster):\n",
        "                dtest = xgb.DMatrix(X_test)\n",
        "                probs = model.predict(dtest)\n",
        "                y_proba = probs if probs.ndim > 1 else np.vstack([1 - probs, probs]).T\n",
        "                y_pred = np.argmax(y_proba, axis=1)\n",
        "            elif lgb is not None and isinstance(model, lgb.Booster):\n",
        "                probs = model.predict(X_test)\n",
        "                y_proba = probs if probs.ndim > 1 else np.vstack([1 - probs, probs]).T\n",
        "                y_pred = np.argmax(y_proba, axis=1)\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction failed for {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    preds.append({\"name\": name, \"y_pred\": y_pred, \"y_proba\": y_proba})\n",
        "\n",
        "print(\"Predictions computed for:\", [p[\"name\"] for p in preds])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Metrics & plots\n",
        "\n",
        "if TASK_TYPE == 'classification':\n",
        "    summary_rows = []\n",
        "    # ROC/PR combined across models\n",
        "    plt.figure(figsize=(6,5))\n",
        "    any_proba = False\n",
        "    for pr in preds:\n",
        "        name = pr['name']\n",
        "        y_pred = pr['y_pred']\n",
        "        y_proba = pr['y_proba']\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        rec_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        rocauc = np.nan\n",
        "        logloss_val = np.nan\n",
        "        if y_proba is not None:\n",
        "            any_proba = True\n",
        "            # binary or multiclass\n",
        "            try:\n",
        "                if y_proba.ndim == 1 or y_proba.shape[1] == 2:\n",
        "                    pos = y_proba[:, -1] if y_proba.ndim > 1 else y_proba\n",
        "                    rocauc = roc_auc_score(y_test, pos)\n",
        "                    fpr, tpr, _ = roc_curve(y_test, pos)\n",
        "                    plt.plot(fpr, tpr, label=f\"{name} (AUC={rocauc:.3f})\")\n",
        "                    try:\n",
        "                        logloss_val = log_loss(y_test, np.vstack([1-pos, pos]).T)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                else:\n",
        "                    rocauc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
        "                    # macro one-vs-rest: approximate by averaging curves of classes\n",
        "                    # For brevity, skip per-class plotting; include name in legend only\n",
        "                    plt.plot([0,1],[0,1],'--', color='gray')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        summary_rows.append({\n",
        "            'model': name,\n",
        "            'accuracy': acc,\n",
        "            'precision_macro': prec_macro,\n",
        "            'recall_macro': rec_macro,\n",
        "            'f1_macro': f1_macro,\n",
        "            'f1_weighted': f1_weighted,\n",
        "            'roc_auc': rocauc,\n",
        "            'log_loss': logloss_val,\n",
        "        })\n",
        "\n",
        "        # Per-class metrics table\n",
        "        try:\n",
        "            rpt = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "            df_rpt = pd.DataFrame(rpt).T.reset_index().rename(columns={'index':'class'})\n",
        "            save_df(f\"per_class_metrics_{name}\", df_rpt)\n",
        "        except Exception as e:\n",
        "            print(f\"classification_report failed for {name}: {e}\")\n",
        "\n",
        "        # Confusion matrices\n",
        "        try:\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            fig, ax = plt.subplots(figsize=(5,4))\n",
        "            im = ax.imshow(cm, cmap='Blues')\n",
        "            ax.set_title(f\"Confusion (counts) - {name}\")\n",
        "            ax.set_xlabel('Predicted')\n",
        "            ax.set_ylabel('True')\n",
        "            plt.colorbar(im, ax=ax)\n",
        "            save_fig(f\"confusion_matrix_counts_{name}\")\n",
        "\n",
        "            cmn = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
        "            fig, ax = plt.subplots(figsize=(5,4))\n",
        "            im = ax.imshow(cmn, cmap='Blues', vmin=0, vmax=1)\n",
        "            ax.set_title(f\"Confusion (normalized) - {name}\")\n",
        "            ax.set_xlabel('Predicted')\n",
        "            ax.set_ylabel('True')\n",
        "            plt.colorbar(im, ax=ax)\n",
        "            save_fig(f\"confusion_matrix_normalized_{name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Confusion matrix plotting failed for {name}: {e}\")\n",
        "\n",
        "        # Precision-Recall curve\n",
        "        if y_proba is not None:\n",
        "            try:\n",
        "                if y_proba.ndim == 1 or y_proba.shape[1] == 2:\n",
        "                    pos = y_proba[:, -1] if y_proba.ndim > 1 else y_proba\n",
        "                    prec, rec, _ = precision_recall_curve(y_test, pos)\n",
        "                    plt.figure(figsize=(6,5))\n",
        "                    plt.plot(rec, prec, label=name)\n",
        "                    plt.xlabel('Recall')\n",
        "                    plt.ylabel('Precision')\n",
        "                    plt.title('Precision-Recall Curve')\n",
        "                    plt.legend()\n",
        "                    save_fig(f\"pr_curve_{name}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Calibration curve\n",
        "        if y_proba is not None:\n",
        "            try:\n",
        "                pos = y_proba[:, -1] if y_proba.ndim > 1 else y_proba\n",
        "                fig, ax = plt.subplots(figsize=(5,4))\n",
        "                prob_true, prob_pred = calibration_curve(y_test, pos, n_bins=10)\n",
        "                CalibrationDisplay(prob_true, prob_pred, pos).plot(ax=ax)\n",
        "                ax.set_title(f\"Calibration - {name}\")\n",
        "                save_fig(f\"calibration_{name}\")\n",
        "                # Brier score\n",
        "                brier = brier_score_loss(y_test, pos)\n",
        "                print(f\"Brier score {name}: {brier:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Calibration failed for {name}: {e}\")\n",
        "\n",
        "    # Save summary table\n",
        "    df_summary = pd.DataFrame(summary_rows)\n",
        "    save_df(\"classification_summary\", df_summary)\n",
        "\n",
        "    # Save combined ROC if any probs\n",
        "    if any_proba:\n",
        "        plt.plot([0,1],[0,1],'--', color='gray')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves (All Models)')\n",
        "        plt.legend()\n",
        "        save_fig('roc_curves_all_models')\n",
        "\n",
        "else:\n",
        "    # Regression branch (metrics & plots)\n",
        "    rows = []\n",
        "    for pr in preds:\n",
        "        name = pr['name']\n",
        "        y_pred = pr['y_pred']\n",
        "        resid = y_test - y_pred\n",
        "        rmse = float(np.sqrt(np.mean(resid**2)))\n",
        "        mae = float(np.mean(np.abs(resid)))\n",
        "        mape_mask = y_test != 0\n",
        "        mape = float(np.mean((np.abs(resid[mape_mask]) / np.abs(y_test[mape_mask])))) if mape_mask.any() else np.nan\n",
        "        r2 = 1 - (np.sum(resid**2) / (np.sum((y_test - np.mean(y_test))**2) + 1e-12))\n",
        "        medae = float(np.median(np.abs(resid)))\n",
        "        rows.append({'model': name, 'rmse': rmse, 'mae': mae, 'mape': mape, 'r2': r2, 'medae': medae})\n",
        "\n",
        "        # Residuals vs fitted\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(y_pred, resid, s=8, alpha=0.6)\n",
        "        plt.axhline(0, color='gray', ls='--')\n",
        "        plt.xlabel('Fitted')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'Residuals vs Fitted - {name}')\n",
        "        save_fig(f\"residuals_vs_fitted_{name}\")\n",
        "\n",
        "        # Pred vs Actual\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(y_test, y_pred, s=8, alpha=0.6)\n",
        "        lims = [min(plt.xlim()[0], plt.ylim()[0]), max(plt.xlim()[1], plt.ylim()[1])]\n",
        "        plt.plot(lims, lims, '--', color='gray')\n",
        "        plt.xlabel('Actual')\n",
        "        plt.ylabel('Predicted')\n",
        "        plt.title(f'Predicted vs Actual - {name}')\n",
        "        save_fig(f\"pred_vs_actual_{name}\")\n",
        "\n",
        "    df_reg = pd.DataFrame(rows)\n",
        "    save_df('regression_summary', df_reg)\n",
        "\n",
        "print('Metrics & plots done.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Feature importance and SHAP (optional)\n",
        "\n",
        "FEATURE_NAMES = list(X_test.columns) if hasattr(X_test, 'columns') else [f'f{i}' for i in range(X_test.shape[1])]\n",
        "\n",
        "for entry, pr in zip(models, preds):\n",
        "    name, model = entry['name'], entry['model']\n",
        "    # Feature importances\n",
        "    try:\n",
        "        imp = None\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            imp = np.asarray(model.feature_importances_)\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            coef = np.asarray(model.coef_)\n",
        "            if coef.ndim == 1:\n",
        "                imp = np.abs(coef)\n",
        "            else:\n",
        "                imp = np.abs(coef).mean(axis=0)\n",
        "        if imp is not None and imp.shape[0] == len(FEATURE_NAMES):\n",
        "            df_imp = pd.DataFrame({'feature': FEATURE_NAMES, 'importance': imp})\\\n",
        "                     .sort_values('importance', ascending=False).head(20)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            plt.barh(df_imp['feature'][::-1], df_imp['importance'][::-1])\n",
        "            plt.title(f'Feature Importance - {name}')\n",
        "            save_fig(f'feature_importance_{name}')\n",
        "            save_df(f'feature_importance_{name}', df_imp)\n",
        "    except Exception as e:\n",
        "        print(f\"Feature importance failed for {name}: {e}\")\n",
        "\n",
        "    # SHAP\n",
        "    if shap is not None:\n",
        "        try:\n",
        "            # Take a small sample for performance\n",
        "            idx = np.random.RandomState(RANDOM_STATE).choice(np.arange(len(X_test)), size=min(1000, len(X_test)), replace=False)\n",
        "            Xs = X_test.iloc[idx] if hasattr(X_test, 'iloc') else X_test[idx]\n",
        "            explainer = None\n",
        "            if hasattr(model, 'predict_proba') and hasattr(model, 'fit'):\n",
        "                # Tree or linear detection is rough; try TreeExplainer fallback to Kernel\n",
        "                try:\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                    shap_values = explainer.shap_values(Xs)\n",
        "                except Exception:\n",
        "                    explainer = shap.KernelExplainer(model.predict_proba, Xs[:100])\n",
        "                    shap_values = explainer.shap_values(Xs[:200])\n",
        "            else:\n",
        "                # boosters\n",
        "                if xgb is not None and isinstance(model, xgb.Booster):\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                    dtest = xgb.DMatrix(Xs)\n",
        "                    shap_values = explainer.shap_values(dtest)\n",
        "                else:\n",
        "                    # generic fallback\n",
        "                    explainer = shap.KernelExplainer(lambda a: pr['y_proba'], Xs[:100])\n",
        "                    shap_values = explainer.shap_values(Xs[:200])\n",
        "\n",
        "            plt.figure(figsize=(6,5))\n",
        "            shap.summary_plot(shap_values, Xs, show=False)\n",
        "            save_fig(f'shap_summary_{name}')\n",
        "        except Exception as e:\n",
        "            print(f\"SHAP skipped for {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5b) Compare models from artifacts/metrics.csv (optional)\n",
        "from pathlib import Path\n",
        "metrics_csv = Path('artifacts/metrics.csv')\n",
        "if metrics_csv.exists():\n",
        "    dfm = pd.read_csv(metrics_csv)\n",
        "    # keep common columns if present\n",
        "    cols = [c for c in ['model','recall','specificity','accuracy','f1','val_auc'] if c in dfm.columns]\n",
        "    if 'model' in cols:\n",
        "        dfm_sorted = dfm.sort_values(by=[c for c in cols if c!='model'][0], ascending=False)\n",
        "        save_df('leaderboard_from_training', dfm_sorted)\n",
        "        # One plot per metric\n",
        "        for m in [c for c in cols if c!='model']:\n",
        "            plt.figure(figsize=(7,4))\n",
        "            plt.bar(dfm_sorted['model'], dfm_sorted[m], color='#2563eb')\n",
        "            plt.ylabel(m)\n",
        "            plt.title(f'Model comparison ({m}) from training metrics')\n",
        "            plt.xticks(rotation=30, ha='right')\n",
        "            save_fig(f'compare_{m}_from_training')\n",
        "    else:\n",
        "        print('metrics.csv missing model column; skipping plots')\n",
        "else:\n",
        "    print('artifacts/metrics.csv not found; skipping training metrics comparison plots')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Model comparison & index\n",
        "\n",
        "# Comparison bars (classification: F1 macro; regression: RMSE)\n",
        "try:\n",
        "    if TASK_TYPE == 'classification':\n",
        "        df = pd.read_csv(TABLES_DIR / 'classification_summary.csv')\n",
        "        df_plot = df[['model','f1_macro','accuracy','roc_auc']]\n",
        "        df_plot = df_plot.set_index('model')\n",
        "        plt.figure(figsize=(7,4))\n",
        "        df_plot['f1_macro'].plot(kind='bar', color='#2563eb')\n",
        "        plt.ylabel('F1 (macro)')\n",
        "        plt.title('Model Comparison - F1 macro')\n",
        "        save_fig('metrics_bar_compare')\n",
        "    else:\n",
        "        df = pd.read_csv(TABLES_DIR / 'regression_summary.csv')\n",
        "        df_plot = df[['model','rmse','r2']]\n",
        "        df_plot = df_plot.set_index('model')\n",
        "        plt.figure(figsize=(7,4))\n",
        "        df_plot['rmse'].plot(kind='bar', color='#2563eb')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.title('Model Comparison - RMSE')\n",
        "        save_fig('metrics_bar_compare')\n",
        "except Exception as e:\n",
        "    print('Comparison plot skipped:', e)\n",
        "\n",
        "# Generate index markdown\n",
        "index_lines = []\n",
        "index_lines.append('# Model Validation Report Index')\n",
        "index_lines.append('')\n",
        "index_lines.append('## Tables')\n",
        "for p in sorted(TABLES_DIR.glob('*.csv')):\n",
        "    index_lines.append(f\"- {p.name}\")\n",
        "index_lines.append('')\n",
        "index_lines.append('## Plots')\n",
        "for p in sorted(PLOTS_DIR.glob('*.png')):\n",
        "    index_lines.append(f\"- {p.name}\")\n",
        "\n",
        "ensure_dir(OUT_DIR)\n",
        "(OUT_DIR / 'README.md').write_text('\\n'.join(index_lines))\n",
        "\n",
        "print('Index written to', OUT_DIR / 'README.md')\n",
        "print('\\n'.join(index_lines))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
